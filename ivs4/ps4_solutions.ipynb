{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universit√§t Heidelberg\n",
    "Distributed Systems I (IVS1)\n",
    "Winter Semester 18/19\n",
    "\n",
    "- Duc Anh Phi\n",
    "- Michael Tabachnik\n",
    "- Edgar Brotzmann\n",
    "\n",
    "# Solutions to Problem Set 4 for lecture Distributed Systems I (IVS1)\n",
    "## Due: 20.11.2018, 2pm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import findspark\n",
    "import re\n",
    "\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Solutions to Problem Set 4 IVS1\") \\\n",
    "    .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-------------------+\n",
      "|InstanceType|ProductDescription|         avg(Price)|\n",
      "+------------+------------------+-------------------+\n",
      "|  r3.2xlarge|        Linux/UNIX| 0.6557668832731297|\n",
      "|   c4.xlarge|           Windows|0.22131742268041452|\n",
      "|   i3.xlarge|           Windows|0.24962053470769482|\n",
      "|  p2.8xlarge|        Linux/UNIX| 117.20000000000049|\n",
      "| x1.16xlarge|           Windows|  9.048597719044308|\n",
      "|  i2.4xlarge|           Windows| 1.5526279912024583|\n",
      "|    c4.large|           Windows|0.10950416912487713|\n",
      "|  i3.8xlarge|        Linux/UNIX| 29.279999999999934|\n",
      "|  r4.2xlarge|        Linux/UNIX|0.21761773442050408|\n",
      "|  c4.2xlarge|           Windows|   0.42304020895996|\n",
      "|  d2.4xlarge|           Windows| 1.1192076448974577|\n",
      "| i3.16xlarge|        Linux/UNIX|   58.5599999999999|\n",
      "|  r3.4xlarge|        Linux/UNIX| 0.5058647696038822|\n",
      "|  m4.4xlarge|           Windows| 1.0006524359704119|\n",
      "|  r4.2xlarge|           Windows| 0.7127052461538115|\n",
      "|  i3.8xlarge|           Windows|               44.0|\n",
      "|  r3.8xlarge|           Windows|  3.896891165579292|\n",
      "|  i3.4xlarge|        Linux/UNIX| 3.4522168725100277|\n",
      "|  r3.4xlarge|           Windows|  4.306813385368257|\n",
      "|  d2.8xlarge|           Windows|   2.37976506728288|\n",
      "+------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def createDataframe(filename):\n",
    "    with gzip.open(filename, 'rt') as f:\n",
    "        # use header as row schema\n",
    "        headerLine = f.readline()\n",
    "        header = re.split(r'\\t+', headerLine.rstrip('\\t\\n'))[1:]\n",
    "        RowSchema = Row(*header)\n",
    "        content = [row for row in f.read().split(\"\\n\") if row]\n",
    "        df = sc.parallelize(content) \\\n",
    "            .map(lambda line: re.split(r'\\t+', line.rstrip('\\t\\n'))[1:]) \\\n",
    "            .map(lambda line: RowSchema(*line)) \\\n",
    "            .toDF()\n",
    "        \n",
    "        return df\n",
    "\n",
    "def convertColumns(df):\n",
    "    df = df.withColumn('Price', df['Price'].cast(DoubleType()))\n",
    "    df = df.withColumn('Timestamp', unix_timestamp('Timestamp', \"yyyy-MM-dd'T'HH:mm:ssZZZZ\"))\n",
    "    return df\n",
    "\n",
    "df = createDataframe(\"prices-ap-northeast-2-2017-11-17.txt.gz\")\n",
    "df = convertColumns(df)\n",
    "df.groupBy('InstanceType', 'ProductDescription').avg('Price').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "#### a)\n",
    "\n",
    "##### What does broadcast provide?\n",
    "\n",
    "Broadcast provide a way to share a readonly variable across multiple machines for task and operations efficiently. It sends the variable to the machines only once where it is cached respectively.\n",
    "\n",
    "##### Which other mechanism does it improve and how?\n",
    "\n",
    "It can reduce number of send data over the network and thus keep communication cost low. An example is shown for the join operation for two datasets. Normally you would directly join both datasets, which shuffles both over the network. A better approach would be, if one dataset is particularly small, to broadcast the smaller dataset as a map to all machines which contain the other dataset, in order to perform the join operation. This way, only the smaller dataset is send over the network.\n",
    "\n",
    "##### Which features of the distributed program determine the number of times the variable will be actually transmitted over the network? Explain the role of tasks and nodes in this.\n",
    "\n",
    "By default, when Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task. The number of times the variable will be actually transmitted over the network depends on the number of tasks which depend on the number of partitions.\n",
    "\n",
    "#### b)\n",
    "\n",
    "##### Describe the function of an accumulator.\n",
    "\n",
    "Often, an application needs to aggregate multiple values as it progresses. \n",
    "Accumulators provide a simple syntax for aggregating values from worker nodes back to the driver program.\n",
    "\n",
    "##### What is the alternative implementation without an accumulator and why is an accumulator a preferred option?\n",
    "\n",
    "A reduce() function can be used instead of an accumulator. It is not preferable because it is more communication costly and cannot aggregate multiple across multiple tasks.\n",
    "\n",
    "##### Which example application for an accumulator is discussed in the video?\n",
    "\n",
    "Overall count of bad records and bad bytes after filtering records. Computing averages.\n",
    "\n",
    "##### What has to be implemented in order to define a custom accumulator?\n",
    "\n",
    "Define an object extending AccumulatorParam\\[T\\], where T is your data type, and tell the system how to work with a custom data type T. Define default value to be initialized for a given T. And define an addInPlace method to merge in values.\n",
    "\n",
    "##### Compare the accumulator mechanism to the reduce()-function\n",
    "\n",
    "The difference is that the reduce function is limited to one variable which is accumulated, whereas with the accumulator mechanism you can define multiple variables which you could accumulate on in parallel, even across multiple tasks.\n",
    "\n",
    "#### c)\n",
    "\n",
    "##### Give three examples of RDD operators that result in RDDs with partitioning. \n",
    "join()\n",
    "mapValues()\n",
    "reduceByKey()\n",
    "\n",
    "##### Explain the connection between partitioning and network traffic.\n",
    "\n",
    "If data is spread across machines arbitrarily (partitions) and if the data has to come together on the same machine there is a lot of network traffic.\n",
    "\n",
    "##### How does the modification on the pageRank example use partitioning to make the code more efficient?\n",
    "\n",
    "Prepartition the links RDD so that links for URLs with the same hash code are on the same node. This saves future shuffling.\n",
    "\n",
    "##### How does Spark exploit the knowledge about the partitioning to save time in task execution?\n",
    "\n",
    "It arranges how data is spread across machines in a way, so that data with the same key is on the same machine.\n",
    "\n",
    "##### How can you create a custom Partitioner?\n",
    "\n",
    "You can define your own subclass of Partitioner to leverage domain-specific knowledge. This subclass has to define the number of partitions, a function to get a partition by key and an 'equals' function to tell whether to partitioners are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "\n",
    "#### a)\n",
    "Any lines typed in the terminal running the netcat server will be counted and printed on screen every second. The output of the streaming program does not behave as expected. It tokenizes the words without removing special characters, thus words with special characters afterwards are counted as unique new words. In the given example string, it counts 'mankind.' and 'man,' as unique words.\n",
    "\n",
    "#### b)\n",
    "The input rate is so high, that the streaming program cannot quite keep up with the throughput. The counting results are not printed every second but rather every few seconds and the printing of counting results continues even after closing the streaming program. \n",
    "After using pv (pipe viewer) to check the throughput of the yes command I get around 300MB/s for 1 character. Assuming one byte per character the yes command is called 300 million times per second.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "#### 1. Describe each step of Spark execution model\n",
    "\n",
    "1. Create DAG of RDDs to represent computation\n",
    "2. Create logical execution plan for DAG\n",
    "3. Schedule and execute individual tasks\n",
    "\n",
    "Step 1: Create RDDs\n",
    "Step 2: Create execution plan\n",
    "pipeline as much as possible\n",
    "split into stages based on need to reorganize data\n",
    "Step 3: Schedule tasks\n",
    "split each stage into tasks\n",
    "a task is data + computation\n",
    "execute all tasks within a stage before moving on\n",
    "each task is delegated to corresponding machine\n",
    "\n",
    "#### 2. In the execution phase, Spark tries to pipeline operations as much as possible. How does pipelining affect performance? Give examples of operations that can be pipelined.\n",
    "\n",
    "#### 3. List the four most common issues described by Aaron. What is the recommended setting and guidelines to deal with the problems described in the talk?\n",
    "\n",
    "#### 4. Transcribe the code given in the talk into the language of your choice. Download the list of last names. Experiment with the number of partitions in the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
