{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universit√§t Heidelberg\n",
    "Distributed Systems I (IVS1)\n",
    "Winter Semester 18/19\n",
    "\n",
    "- Duc Anh Phi\n",
    "- Michael Tabachnik\n",
    "- Edgar Brotzmann\n",
    "\n",
    "# Solutions to Problem Set 4 for lecture Distributed Systems I (IVS1)\n",
    "## Due: 20.11.2018, 2pm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import findspark\n",
    "import re\n",
    "\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Solutions to Problem Set 4 IVS1\") \\\n",
    "    .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-------------------+\n",
      "|InstanceType|ProductDescription|         avg(Price)|\n",
      "+------------+------------------+-------------------+\n",
      "|  r3.2xlarge|        Linux/UNIX| 0.6557668832731297|\n",
      "|   c4.xlarge|           Windows|0.22131742268041452|\n",
      "|   i3.xlarge|           Windows|0.24962053470769482|\n",
      "|  p2.8xlarge|        Linux/UNIX| 117.20000000000049|\n",
      "| x1.16xlarge|           Windows|  9.048597719044308|\n",
      "|  i2.4xlarge|           Windows| 1.5526279912024583|\n",
      "|    c4.large|           Windows|0.10950416912487713|\n",
      "|  i3.8xlarge|        Linux/UNIX| 29.279999999999934|\n",
      "|  r4.2xlarge|        Linux/UNIX|0.21761773442050408|\n",
      "|  c4.2xlarge|           Windows|   0.42304020895996|\n",
      "|  d2.4xlarge|           Windows| 1.1192076448974577|\n",
      "| i3.16xlarge|        Linux/UNIX|   58.5599999999999|\n",
      "|  r3.4xlarge|        Linux/UNIX| 0.5058647696038822|\n",
      "|  m4.4xlarge|           Windows| 1.0006524359704119|\n",
      "|  r4.2xlarge|           Windows| 0.7127052461538115|\n",
      "|  i3.8xlarge|           Windows|               44.0|\n",
      "|  r3.8xlarge|           Windows|  3.896891165579292|\n",
      "|  i3.4xlarge|        Linux/UNIX| 3.4522168725100277|\n",
      "|  r3.4xlarge|           Windows|  4.306813385368257|\n",
      "|  d2.8xlarge|           Windows|   2.37976506728288|\n",
      "+------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def createDataframe(filename):\n",
    "    with gzip.open(filename, 'rt') as f:\n",
    "        # use header as row schema\n",
    "        headerLine = f.readline()\n",
    "        header = re.split(r'\\t+', headerLine.rstrip('\\t\\n'))[1:]\n",
    "        RowSchema = Row(*header)\n",
    "        content = [row for row in f.read().split(\"\\n\") if row]\n",
    "        df = sc.parallelize(content) \\\n",
    "            .map(lambda line: re.split(r'\\t+', line.rstrip('\\t\\n'))[1:]) \\\n",
    "            .map(lambda line: RowSchema(*line)) \\\n",
    "            .toDF()\n",
    "        \n",
    "        return df\n",
    "\n",
    "def convertColumns(df):\n",
    "    df = df.withColumn('Price', df['Price'].cast(DoubleType()))\n",
    "    df = df.withColumn('Timestamp', unix_timestamp('Timestamp', \"yyyy-MM-dd'T'HH:mm:ssZZZZ\"))\n",
    "    return df\n",
    "\n",
    "df = createDataframe(\"prices-ap-northeast-2-2017-11-17.txt.gz\")\n",
    "df = convertColumns(df)\n",
    "df.groupBy('InstanceType', 'ProductDescription').avg('Price').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "#### a)\n",
    "\n",
    "##### What does broadcast provide?\n",
    "\n",
    "Broadcast provide a way to share a readonly variable across multiple machines for task and operations efficiently. It sends the variable to the machines only once where it is cached respectively.\n",
    "\n",
    "##### Which other mechanism does it improve and how?\n",
    "\n",
    "It can reduce number of send data over the network and thus keep communication cost low. An example is shown for the join operation for two datasets. Normally you would directly join both datasets, which shuffles both over the network. A better approach would be, if one dataset is particularly small, to broadcast the smaller dataset as a map to all machines which contain the other dataset, in order to perform the join operation. This way, only the smaller dataset is send over the network.\n",
    "\n",
    "##### Which features of the distributed program determine the number of times the variable will be actually transmitted over the network? Explain the role of tasks and nodes in this.\n",
    "\n",
    "By default, when Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task. The number of times the variable will be actually transmitted over the network depends on the number of tasks which depend on the number of partitions. However, if a variable is broadcasted, it is only send once for each node in the cluster and is going to be cached respectively.\n",
    "\n",
    "#### b)\n",
    "\n",
    "##### Describe the function of an accumulator.\n",
    "\n",
    "Often, an application needs to aggregate multiple values as it progresses. \n",
    "Accumulators provide a simple syntax for aggregating values from worker nodes back to the driver program.\n",
    "\n",
    "##### What is the alternative implementation without an accumulator and why is an accumulator a preferred option?\n",
    "\n",
    "You could use the reduce() function alternatively.\n",
    "The problem with reduce() is that you only get the result after all data has been processed. In contrast the accumulator mechanism gets real-time updates on its accumulator variable from each worker node. It can also aggregate multiple variables across multiple tasks.\n",
    "\n",
    "##### Which example application for an accumulator is discussed in the video?\n",
    "\n",
    "Overall count of bad records and bad bytes after filtering records. Additionally it can be used for computing averages.\n",
    "\n",
    "##### What has to be implemented in order to define a custom accumulator?\n",
    "\n",
    "Define an object extending AccumulatorParam\\[T\\], where T is your data type, and tell the system how to work with a custom data type T. Define default value to be initialized for a given T. And define an addInPlace method to merge in values.\n",
    "\n",
    "##### Compare the accumulator mechanism to the reduce()-function\n",
    "\n",
    "The reduce()-function collects data from all RDDs and reduces them to one value. The problem is that you only get the result after all data has been processed. In contrast the accumulator mechanism gets real-time updates on its accumulator variable from each worker node.\n",
    "Additionally the reduce function is limited to one variable which is accumulated, whereas with the accumulator mechanism you can define multiple variables which you could accumulate on in parallel, even across multiple tasks.\n",
    "\n",
    "#### c)\n",
    "\n",
    "##### Give three examples of RDD operators that result in RDDs with partitioning. \n",
    "- join()\n",
    "- mapValues()\n",
    "- reduceByKey()\n",
    "\n",
    "##### Explain the connection between partitioning and network traffic.\n",
    "\n",
    "If data is spread across machines arbitrarily (partitions) and if the data has to come together on the same machine there is a lot of network traffic.\n",
    "\n",
    "##### How does the modification on the pageRank example use partitioning to make the code more efficient?\n",
    "\n",
    "Prepartition the links RDD so that links for URLs with the same hash code are on the same node. This saves future shuffling.\n",
    "\n",
    "##### How does Spark exploit the knowledge about the partitioning to save time in task execution?\n",
    "\n",
    "It arranges how data is spread across machines in a way, so that data with the same key is on the same machine. This reduces the amount of data which needs to be send over the network, which then saves time in task execution because the tasks receive the data quicker.\n",
    "\n",
    "##### How can you create a custom Partitioner?\n",
    "\n",
    "You can define your own subclass of Partitioner to leverage domain-specific knowledge. This subclass must contain three function:\n",
    "\n",
    "- numPartitions: defines the number of partitions\n",
    "- getPartition: get a partition by key\n",
    "- equals: tell whether two partitions are equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "\n",
    "#### a)\n",
    "Any lines typed in the terminal running the netcat server will be counted and printed on screen every second. The output of the streaming program does not behave as expected. It tokenizes the words without removing special characters, thus words with special characters afterwards are counted as unique new words. In the given example string, it counts 'mankind.' and 'man,' as unique words.\n",
    "\n",
    "#### b)\n",
    "The input rate is so high, that the streaming program cannot quite keep up with the throughput. The counting results are not printed every second but rather every few seconds and the printing of counting results continues even after closing the streaming program. \n",
    "After using *pv* (pipe viewer) to check the throughput of the *yes*-command I get around 300MB/s for 1 character. Assuming one byte per character the *yes*-command is called 300 million times per second.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "#### 1. Describe each step of Spark execution model\n",
    "\n",
    "##### First Step: Create DAG of RDDs to represent computation\n",
    "\n",
    "##### Second Step: Create logical execution plan for DAG\n",
    "\n",
    "- pipeline as much as possible\n",
    "- split into \"stages\" based on need to reorganize data\n",
    "\n",
    "##### Third Step: Schedule and execute individual tasks\n",
    "\n",
    "- split each stage into tasks\n",
    "- a task is data + computation\n",
    "- execute all tasks within a stage before moving on\n",
    "- each task is delegated to corresponding machine\n",
    "\n",
    "#### 2. In the execution phase, Spark tries to pipeline operations as much as possible. How does pipelining affect performance? Give examples of operations that can be pipelined.\n",
    "\n",
    "A high level of pipelining combined with multiple threads can result in high performance, because the threads can run independent and without the necessity of waiting for other data. In addition, thread-scheduling is much easier: If one task is finished, the processor can easily run the next enqueued task. In the presentation, the operations ‚Äúmap‚Äù and the operations ‚ÄúgroupBy‚Äù, ‚ÄúmapValues‚Äù and ‚Äúcollect‚Äù can be pipelined.\n",
    "\n",
    "\n",
    "#### 3. List the four most common issues described by Aaron. What is the recommended setting and guidelines to deal with the problems described in the talk?\n",
    "\n",
    "1.\tEnsure enough partitions for concurrency\n",
    "2.\tMinimize memory consumption (especially of sorting and large keys in groupBy‚Äôs)\n",
    "3.\tMinimize amount of data shuffled\n",
    "4.\tKnow the standard library\n",
    "\n",
    "So, 1. and 2. are about tuning the number of partitions: The recommended lower bound is about 2 x number of cores in the cluster and the upper bound is depending on the execution time for each task (at least 100 ms). A \"reasonable number\" of partitions is commonly between 100 and 10 000 partitions.\n",
    "\n",
    "To face memory problems, you can increase spark.executor.memory or the number of partitions, or you can re-evaluate your program structure.\n",
    "\n",
    "#### 4. Transcribe the code given in the talk into the language of your choice. Download the list of last names. Experiment with the number of partitions in the second version: what number of partition yield the fastest results on your computer?\n",
    "\n",
    "As you can see below, we get the best result with 2 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 74.23580408096313 seconds\n"
     ]
    }
   ],
   "source": [
    "# Naive example\n",
    "start_time = time.time()\n",
    "sc.textFile(\"last-names.csv\") \\\n",
    "    .map(lambda name: (name[0], name)) \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(lambda names: len(names)) \\\n",
    "    .collect()\n",
    "print(\"took {} seconds\".format(time.time() - start_time))\n",
    "# took 74.23580408096313 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 65.30063247680664 seconds\n"
     ]
    }
   ],
   "source": [
    "# 'Optimized' example with 1 partition\n",
    "start_time = time.time()\n",
    "sc.textFile(\"last-names.csv\") \\\n",
    "    .distinct(numPartitions = 1) \\\n",
    "    .map(lambda name: (name[0], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "print(\"took {} seconds\".format(time.time() - start_time))\n",
    "# took 65.30063247680664 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Optimized' example with 2 partitions (fastest)\n",
    "start_time = time.time()\n",
    "sc.textFile(\"last-names.csv\") \\\n",
    "    .distinct(numPartitions = 2) \\\n",
    "    .map(lambda name: (name[0], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "print(\"took {} seconds\".format(time.time() - start_time))\n",
    "# took 63.04784870147705 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Optimized' example with 3 partitions\n",
    "start_time = time.time()\n",
    "sc.textFile(\"last-names.csv\") \\\n",
    "    .distinct(numPartitions = 3) \\\n",
    "    .map(lambda name: (name[0], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "print(\"took {} seconds\".format(time.time() - start_time))\n",
    "# took 64.92784357070923 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Optimized' example with 4 partitions\n",
    "start_time = time.time()\n",
    "sc.textFile(\"last-names.csv\") \\\n",
    "    .distinct(numPartitions = 4) \\\n",
    "    .map(lambda name: (name[0], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "print(\"took {} seconds\".format(time.time() - start_time))\n",
    "# took 63.1761691570282 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Optimized' example with 6 partitions\n",
    "start_time = time.time()\n",
    "sc.textFile(\"last-names.csv\") \\\n",
    "    .distinct(numPartitions = 6) \\\n",
    "    .map(lambda name: (name[0], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "print(\"took {} seconds\".format(time.time() - start_time))\n",
    "# took 64.52375602722168 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Optimized' example with 8 partitions\n",
    "start_time = time.time()\n",
    "sc.textFile(\"last-names.csv\") \\\n",
    "    .distinct(numPartitions = 8) \\\n",
    "    .map(lambda name: (name[0], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()\n",
    "print(\"took {} seconds\".format(time.time() - start_time))\n",
    "# took 65.26732587814331 seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
