{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universit√§t Heidelberg\n",
    "Distributed Systems I (IVS1)\n",
    "Winter Semester 18/19\n",
    "\n",
    "- Duc Anh Phi\n",
    "- Michael Tabachnik\n",
    "- Edgar Brotzmann\n",
    "\n",
    "\n",
    "# Solutions to Problem Set 5 for lecture Distributed Systems I (IVS1)\n",
    "## Due: 27.11.2018, 2pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "#### a)\n",
    "\n",
    "##### What are the vector data type used for?\n",
    "\n",
    "The vector data type is used for representing data locally on the machine.\n",
    "\n",
    "##### What is the difference between the representation used on sparse and dense vector?\n",
    "\n",
    "A dense vector is backed by a double array representing its entry values, while a sparse vector is backed by two parallel arrays: indices and values, corresponding to nonzero entries. The sparse vector is prefered over a dense vector if the represented data has a lot of zero entries, since it ignores zero entries which saves memory space.\n",
    "\n",
    "#### b)\n",
    "\n",
    "##### Come up with 2 new features - why would such new features might be relevant for the prediction?\n",
    "\n",
    "Number of rooms per person: We assume the more rooms a inhabitant can afford it is more likely that the house value is going to be higher.\n",
    "\n",
    "Number of people per household: The living density of a household might be a good indicator for the prediction.\n",
    "\n",
    "\n",
    "#### c)\n",
    "\n",
    "##### Run code with newly selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import findspark\n",
    "import re\n",
    "\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Solutions to Problem Set 5 IVS1\") \\\n",
    "    .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataframe():\n",
    "    rdd = sc.textFile('cal_housing.data')\n",
    "    header = sc.textFile('cal_housing.domain')\n",
    "    df = rdd \\\n",
    "    .map(lambda line: line.split(',')) \\\n",
    "    .map(lambda line: Row(\n",
    "        longitude=line[0], \n",
    "        latitude=line[1], \n",
    "        housingMedianAge=line[2],\n",
    "        totalRooms=line[3],\n",
    "        totalBedRooms=line[4],\n",
    "        population=line[5], \n",
    "        households=line[6],\n",
    "        medianIncome=line[7],\n",
    "        medianHouseValue=line[8])) \\\n",
    "    .toDF()  \n",
    "    return df\n",
    "\n",
    "def convertColumns(df, names, newType):\n",
    "    for name in names:\n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df\n",
    "\n",
    "columns = [\n",
    "    'households',\n",
    "    'housingMedianAge',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'medianHouseValue',\n",
    "    'medianIncome',\n",
    "    'population',\n",
    "    'totalBedRooms',\n",
    "    'totalRooms'\n",
    "]\n",
    "\n",
    "df = createDataframe()\n",
    "df = convertColumns(df, columns, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the values of `medianHouseValue`\n",
    "df = df.withColumn(\"medianHouseValue\", col(\"medianHouseValue\")/100000)\n",
    "\n",
    "# Add the new columns to `df`\n",
    "df = df \\\n",
    "    .withColumn(\"roomsPerPerson\", col(\"totalRooms\")/col(\"population\")) \\\n",
    "    .withColumn(\"personPerHousehold\", col(\"population\")/col(\"households\"))\n",
    "\n",
    "# Re-order and select columns\n",
    "df = df.select(\n",
    "    \"medianHouseValue\", \n",
    "    \"totalBedRooms\", \n",
    "    \"population\", \n",
    "    \"households\", \n",
    "    \"medianIncome\", \n",
    "    \"personPerHousehold\", \n",
    "    \"roomsPerPerson\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the `input_data` \n",
    "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Replace `df` with the new DataFrame\n",
    "df = spark.createDataFrame(input_data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the `standardScaler`\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "# Fit the DataFrame to the scaler\n",
    "scaler = standardScaler.fit(df)\n",
    "\n",
    "# Transform the data in `df` with the scaler\n",
    "scaled_df = scaler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize `lr`\n",
    "lr = LinearRegression(labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8764169349058016\n",
      "R2: 0.42297586848753665\n",
      "Coefficients: [0.0,0.0,0.0,0.27982347885597,0.0,0.0]\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predicted = linearModel.transform(test_data)\n",
    "\n",
    "# Extract the predictions and the \"known\" correct labels\n",
    "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
    "\n",
    "# Zip `predictions` and `labels` into a list\n",
    "predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "# Print out first 5 instances of `predictionAndLabel` \n",
    "predictionAndLabel[:5]\n",
    "\n",
    "# Get the RMSE\n",
    "print('RMSE: {}'.format(linearModel.summary.rootMeanSquaredError))\n",
    "\n",
    "# Get the R2\n",
    "print('R2: {}'.format(linearModel.summary.r2))\n",
    "\n",
    "# Get the coefficients\n",
    "print('Coefficients: {}'.format(linearModel.coefficients))\n",
    "\n",
    "# RMSE: 0.8764169349058016\n",
    "# R2: 0.42297586848753665\n",
    "# Coefficients: [0.0,0.0,0.0,0.27982347885597,0.0,0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What features have most influence on the prediction?\n",
    "The median income had the most influence on the prediction. If you look at the coefficients the model only used this feature, ignoring all other features: the coefficient for the median income has a high value whereas all other coefficients are zero.\n",
    "\n",
    "##### How can one measure the quality of a linear regression model prediction?\n",
    "One can print out the root mean squared error (RMSE), which represents the difference between the predicted and known value. The smaller an RMSE value, the better the quality of the model.\n",
    "\n",
    "Additionally one can print out the coefficient of the determination (R2), which represents how close the data are to the fitted regression line. The higher R2, the better the model fits the data.\n",
    "\n",
    "#### d)\n",
    "##### Modify current implementation to use spark pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[label: double, features: vector, features_scaled: vector, prediction: double]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "standScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "linearReg = LinearRegression(labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "pipeline = Pipeline(stages=[standScaler, linearReg])\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "# Generate predictions\n",
    "predicted = model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e)\n",
    "\n",
    "##### Take a look at the example code on how to use CrossValidation and incorporate it into your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(linearReg.regParam, [0.3, 0.1, 0.01]) \\\n",
    "    .addGrid(linearReg.fitIntercept, [False, True]) \\\n",
    "    .addGrid(linearReg.elasticNetParam, [0.0, 0.5, 0.8, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=2)\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f)\n",
    "\n",
    "##### Run your model with the crossvalidation algorithm. What is the best model configuration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8058348969098945\n",
      "R2: 0.5123204275213011\n",
      "Coefficients: [-4.9584499901162605e-05,-0.0004184279464709144,0.0013391432868942483,0.41127157591553165,0.0004510329315685896,0.00460736475294319]\n"
     ]
    }
   ],
   "source": [
    "bestModel = cvModel.bestModel.stages[-1]\n",
    "\n",
    "# Get the RMSE\n",
    "print('RMSE: {}'.format(bestModel.summary.rootMeanSquaredError))\n",
    "\n",
    "# Get the R2\n",
    "print('R2: {}'.format(bestModel.summary.r2))\n",
    "\n",
    "# Get the coefficients\n",
    "print('Coefficients: {}'.format(bestModel.coefficients))\n",
    "\n",
    "# Performance after tuning\n",
    "# RMSE: 0.8058348969098945\n",
    "# R2: 0.5123204275213011\n",
    "# Coefficients: [\n",
    "#    -4.9584499901162605e-05,\n",
    "#    -0.0004184279464709144,\n",
    "#    0.0013391432868942483,\n",
    "#    0.41127157591553165,\n",
    "#    0.0004510329315685896,\n",
    "#    0.00460736475294319\n",
    "#]\n",
    "\n",
    "# Performance before tuning\n",
    "# RMSE: 0.8764169349058016\n",
    "# R2: 0.42297586848753665\n",
    "# Coefficients: [0.0,0.0,0.0,0.27982347885597,0.0,0.0]\n",
    "\n",
    "# As you can see after the tuning our model performs better.\n",
    "# It takes every input feature into account instead of ignoring\n",
    "# all except for the median income feature.\n",
    "# This results into a lower RMSE and a higher R2 value for the tuned model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "#### a) What strengths and weaknesses do SOAP/WSDL-based web services have?\n",
    "\n",
    "#### b) What is your own opinion with regards to strengths and weaknesses of RESTful Web Services?\n",
    "\n",
    "#### c) Which decisions do software architects / developers have to make in the case of SOAP/WSDL-based web services and which ones in the case of RESTful web services?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "#### A realtime online multiplayer game:\n",
    "- realtime data transfer\n",
    "- custom application layer protocol on top of tcp/ip\n",
    "\n",
    "#### Incorporate current exchange rates for international currencies in a desktop calculator app\n",
    "\n",
    "#### A cross-platform distributed file system\n",
    "\n",
    "#### A gaming platform where human and AI contestants are supposed to compete in turn-based games (like chess) that are ruled and recorded by a central server\n",
    "\n",
    "#### A service that replies to messages including a bitmap image with a set of labels that apply for the content of the image\n",
    "\n",
    "#### A distributed computing platform that runs on a diverse grid of machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Why simple solutions tend to perdude in industry when complex approaches seem to dominate academia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
